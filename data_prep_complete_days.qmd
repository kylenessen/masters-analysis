---
title: "Data Preparation - Complete Days Analysis"
format:
  html:
    toc: true
    number-sections: false
execute:
  echo: true
  warning: false
  message: false
---

# Complete Days Data Preparation Pipeline

This script consolidates all data preparation steps for complete days analysis, 
combining butterfly abundance, wind data, temperature, and deployment metadata.

```{r}
#| label: setup
#| include: true

# === CONFIGURATION ===
SHOW_VALIDATION_PLOTS <- TRUE  # Toggle diagnostic plots during development
WIND_THRESHOLD <- 2  # Wind threshold for features (m/s)

# === LIBRARIES ===
library(jsonlite)
library(dplyr)
library(purrr)
library(tidyr)
library(stringr)
library(tibble)
library(ggplot2)
library(lubridate)
library(furrr)
library(DBI)
library(RSQLite)
library(readr)
library(here)
library(glue)

theme_set(theme_minimal(base_size = 12))

# Local timezone for parsing/display
local_tz <- "America/Los_Angeles"
```

# Step 1: Combine Deployment Data

```{r}
#| label: combine-deployments

# Read QGIS data
qgis_data <- list()
if (file.exists('data/deployments/deployments_QGIS.csv')) {
  qgis_raw <- read_csv('data/deployments/deployments_QGIS.csv', show_col_types = FALSE)
  for (i in 1:nrow(qgis_raw)) {
    deployment_id <- qgis_raw$deployment_id[i]
    if (!is.na(deployment_id) && deployment_id != "") {
      qgis_data[[deployment_id]] <- qgis_raw[i, ]
    }
  }
}

# Read label data
label_data <- list()
if (file.exists('data/deployments/deployments_label.csv')) {
  label_raw <- read_csv('data/deployments/deployments_label.csv', show_col_types = FALSE)
  for (i in 1:nrow(label_raw)) {
    deployment_id <- label_raw$`Deployment ID`[i]
    if (!is.na(deployment_id) && deployment_id != "") {
      label_data[[deployment_id]] <- label_raw[i, ]
    }
  }
}

# Get all unique deployment IDs
all_ids <- union(names(qgis_data), names(label_data))

# Define output columns
qgis_columns <- c('camera_name', 'wind_meter_name', 'Deployed_time', 'Recovered_time', 
                  'notes', 'height_m', 'horizontal_dist_to_cluster_m', 'view_direction', 
                  'cluster_count', 'deployment_id', 'status', 'photo_interval_min', 
                  'monarchs_present', 'youtube_url', 'latitude', 'longitude')

# Create combined deployments data
deployments_list <- list()

for (deployment_id in sort(all_ids)) {
  combined_row <- tibble(deployment_id = deployment_id)
  
  # Add QGIS data
  if (deployment_id %in% names(qgis_data)) {
    qgis_row <- qgis_data[[deployment_id]]
    for (col in qgis_columns) {
      if (col %in% names(qgis_row)) {
        combined_row[[col]] <- qgis_row[[col]]
      } else {
        combined_row[[col]] <- NA
      }
    }
  } else {
    # Fill with NA values if not in QGIS
    for (col in qgis_columns) {
      if (col != "deployment_id") {
        combined_row[[col]] <- NA
      }
    }
  }
  
  # Add label data with renamed columns to avoid conflicts
  if (deployment_id %in% names(label_data)) {
    label_row <- label_data[[deployment_id]]
    combined_row$label_status <- label_row$Status %||% NA
    combined_row$`Percent Complete` <- label_row$`Percent Complete` %||% NA
    combined_row$Observer <- label_row$Observer %||% NA
    combined_row$Effort <- label_row$Effort %||% NA
    combined_row$label_notes <- label_row$Notes %||% NA
    combined_row$label_youtube_url <- label_row$`Youtube Link` %||% NA
    combined_row$view_id <- label_row$`View ID` %||% NA
  } else {
    # Fill with NA values if not in label data
    combined_row$label_status <- NA
    combined_row$`Percent Complete` <- NA
    combined_row$Observer <- NA
    combined_row$Effort <- NA
    combined_row$label_notes <- NA
    combined_row$label_youtube_url <- NA
    combined_row$view_id <- NA
  }
  
  deployments_list[[deployment_id]] <- combined_row
}

deployments <- bind_rows(deployments_list)

cat("Combined", nrow(deployments), "deployment records\n")
```

# Step 2: Extract Wind Data

```{r}
#| label: extract-wind-data

# Helper: available wind DBs
wind_db_dir <- "data/wind"
db_files <- tibble(
  db_path = list.files(wind_db_dir, pattern = "\\.s3db$", full.names = TRUE)
) %>% 
  mutate(
    wind_meter_name = tools::file_path_sans_ext(basename(db_path))
  )

cat("Found", nrow(db_files), "wind database files\n")

# Function to extract all wind data from a database
fetch_all_wind <- function(db_path) {
  con <- dbConnect(RSQLite::SQLite(), db_path)
  on.exit(dbDisconnect(con), add = TRUE)
  
  q <- "select time, speed, gust from Wind order by time"
  dbGetQuery(con, q) %>%
    mutate(
      time = ymd_hms(time, tz = local_tz, quiet = TRUE),
      speed = as.numeric(speed),
      gust = as.numeric(gust)
    )
}

# Extract and combine all wind data
if (nrow(db_files) > 0) {
  wind_all <- db_files %>%
    mutate(data = map(db_path, fetch_all_wind)) %>%
    unnest(data) %>%
    relocate(wind_meter_name, .before = time) %>%
    select(-db_path)
  
  cat("Extracted", nrow(wind_all), "wind records from", n_distinct(wind_all$wind_meter_name), "meters\n")
} else {
  wind_all <- tibble(wind_meter_name = character(), time = as.POSIXct(character()), 
                     speed = numeric(), gust = numeric())
  cat("No wind data found\n")
}
```

# Step 3: Calculate Butterfly Abundance

```{r}
#| label: butterfly-abundance-setup

deploy_dir <- "data/deployments"
json_files <- list.files(deploy_dir, pattern = "\\.json$", full.names = TRUE)

cat("Found", length(json_files), "JSON deployment files\n")

# Null-coalescing helper
or_else <- function(x, default) {
  if (is.null(x)) default else x
}

# Vectorized count value mapping
count_min_value_vec <- function(x) {
  # Initialize result vector
  result <- rep(NA_real_, length(x))
  
  # Handle NULL and NA values
  null_mask <- sapply(x, is.null)
  result[null_mask] <- NA_real_
  
  # Process non-NULL values
  non_null <- which(!null_mask)
  if (length(non_null) == 0) {
    return(result)
  }
  
  x_non_null <- x[non_null]
  
  # Handle numeric values directly (vectorized)
  is_num <- sapply(x_non_null, is.numeric)
  num_idx <- non_null[is_num]
  if (length(num_idx) > 0) {
    result[num_idx] <- as.numeric(unlist(x_non_null[is_num]))
  }
  
  # Process non-numeric values
  char_idx <- non_null[!is_num]
  if (length(char_idx) == 0) {
    return(result)
  }
  
  chars <- trimws(as.character(unlist(x_non_null[!is_num])))
  char_result <- rep(NA_real_, length(chars))
  
  # Exact zero
  char_result[chars == "0"] <- 0
  
  # Range patterns (e.g., "1-9", "10-99")
  range_match <- str_match(chars, "^(\\d+)-(\\d+)$")
  has_range <- !is.na(range_match[, 1])
  char_result[has_range] <- as.numeric(range_match[has_range, 2])
  
  # Plus patterns (e.g., "1000+")
  plus_match <- str_match(chars, "^(\\d+)\\+$")
  has_plus <- !is.na(plus_match[, 1])
  char_result[has_plus] <- as.numeric(plus_match[has_plus, 2])
  
  # Try numeric coercion for remaining
  remaining <- is.na(char_result) & chars != ""
  if (any(remaining)) {
    suppressWarnings({
      nums <- as.numeric(chars[remaining])
    })
    char_result[remaining] <- nums
  }
  
  result[char_idx] <- char_result
  return(result)
}

extract_cells_df <- function(cells) {
  if (is.null(cells) || length(cells) == 0) {
    return(tibble())
  }
  
  # cells is a named list of cell_ij -> list(count=..., directSun/sunlight=...)
  keys <- names(cells)
  vals <- cells
  
  # Extract all values at once
  counts <- map(vals, ~ or_else(.x[["count"]], NA))
  
  # Vectorized direct_sun extraction
  direct_sun <- map_lgl(vals, function(.x) {
    if (!is.null(.x[["directSun"]])) {
      isTRUE(.x[["directSun"]])
    } else if (!is.null(.x[["sunlight"]])) {
      isTRUE(.x[["sunlight"]])
    } else {
      FALSE
    }
  })
  
  tibble(
    cell = keys,
    count_raw = counts,
    direct_sun = direct_sun,
    count_min = count_min_value_vec(counts)
  )
}

parse_deployment <- function(file) {
  deployment_id <- tools::file_path_sans_ext(basename(file))
  x <- jsonlite::read_json(file, simplifyVector = FALSE)
  
  # Determine structure
  if (is.list(x) && !is.null(x$classifications)) {
    cls <- x$classifications
  } else {
    cls <- x
  }
  
  if (length(cls) == 0) {
    return(tibble(
      deployment_id = deployment_id,
      image_filename = NA_character_,
      total_butterflies = NA_real_,
      butterflies_direct_sun = NA_real_
    ))
  }
  
  # Build data frame all at once using list columns
  im_names <- names(cls)
  
  res <- tibble(
    deployment_id = deployment_id,
    image_filename = im_names,
    im_obj = cls
  ) %>%
    # Filter out night frames
    filter(!map_lgl(im_obj, ~ isTRUE(or_else(.x[["isNight"]], FALSE)))) %>%
    mutate(
      # Extract cells data
      cells_df = map(im_obj, ~ extract_cells_df(or_else(.x$cells, NULL))),
      # Calculate totals
      total_butterflies = map_dbl(cells_df, function(df) {
        if (nrow(df) == 0) NA_real_ else sum(df$count_min, na.rm = TRUE)
      }),
      butterflies_direct_sun = map_dbl(cells_df, function(df) {
        if (nrow(df) == 0) NA_real_ else sum(df$count_min[df$direct_sun], na.rm = TRUE)
      })
    ) %>%
    select(-im_obj, -cells_df)
  
  res
}
```

```{r}
#| label: process-butterfly-data

# Set up parallel processing
plan(multisession, workers = min(4, length(json_files)))

# Parse all deployments in parallel
abundance_index <- future_map_dfr(json_files, function(f) {
  tryCatch(parse_deployment(f), error = function(e) {
    warning(sprintf("Failed to parse %s: %s", f, conditionMessage(e)))
    tibble(
      deployment_id = tools::file_path_sans_ext(basename(f)),
      image_filename = NA_character_,
      total_butterflies = NA_real_,
      butterflies_direct_sun = NA_real_
    )
  })
}, .options = furrr_options(seed = TRUE)) %>%
  arrange(deployment_id, image_filename)

# Close parallel workers
plan(sequential)

cat("Processed", nrow(abundance_index), "butterfly observations from", 
    n_distinct(abundance_index$deployment_id), "deployments\n")
```

```{r}
#| label: filter-and-downsample

# Pre-compute night intervals as POSIXct for vectorized filtering
parse_ts <- function(s) as.POSIXct(strptime(s, "%Y%m%d%H%M%S", tz = "UTC"))

night_periods <- list(
  SC1 = list(
    list(start = "20231117174001", end = "20231118062001"),
    list(start = "20231118172501", end = "20231119061501"),
    list(start = "20231119171001", end = "20231120062001"),
    list(start = "20231120172001", end = "20231121063001")
  ),
  SC2 = list(
    list(start = "20231117172501", end = "20231118062001"),
    list(start = "20231118171501", end = "20231119061501")
  )
)

# Pre-compute intervals
night_intervals <- map(night_periods, function(periods) {
  map(periods, ~ list(
    start = parse_ts(.x$start),
    end = parse_ts(.x$end)
  ))
})

# Vectorized night checking
is_night_vec <- function(ids, timestamps) {
  result <- rep(FALSE, length(ids))
  
  for (dep_id in names(night_intervals)) {
    mask <- ids == dep_id & !is.na(timestamps)
    if (!any(mask)) next
    
    dep_ts <- timestamps[mask]
    is_night_mask <- rep(FALSE, sum(mask))
    
    for (period in night_intervals[[dep_id]]) {
      is_night_mask <- is_night_mask | (dep_ts >= period$start & dep_ts <= period$end)
    }
    
    result[mask] <- is_night_mask
  }
  
  result
}

# Apply filters with vectorized operations
abundance_index_filtered <- abundance_index %>%
  mutate(
    timestamp_str = stringr::str_match(tidyr::replace_na(image_filename, ""), ".*_(\\d{14})")[, 2],
    timestamp = parse_ts(timestamp_str)
  ) %>%
  filter(!is.na(timestamp)) %>%
  # Vectorized night filtering for SC1/SC2
  filter(!is_night_vec(deployment_id, timestamp)) %>%
  # Downsampling: SC1/SC2 from 5-min to 30-min intervals
  filter(!(deployment_id %in% c("SC1", "SC2") & (lubridate::minute(timestamp) %% 30 != 0))) %>%
  # Downsampling: SC12, SC9, SLC6_2 from 10-min to 30-min intervals (keep 00, 30 minutes only)
  filter(!(deployment_id %in% c("SC12", "SC9", "SLC6_2") & !(lubridate::minute(timestamp) %in% c(0, 30)))) %>%
  # Add day_id column - combining deployment_id and date
  mutate(
    date = as.Date(timestamp),
    day_id = paste0(deployment_id, "-", format(date, "%Y%m%d"))
  ) %>%
  # Filter out days where ALL observations have zero butterflies
  group_by(day_id) %>%
  filter(!(all(total_butterflies == 0 | is.na(total_butterflies)))) %>%
  ungroup()

cat("After filtering:", nrow(abundance_index_filtered), "observations remaining\n")
```

```{r}
#| label: complete-days-logic

# Add complete_day column - only TRUE for middle days with records before AND after
# First get unique days per deployment
unique_days_per_deployment <- abundance_index_filtered %>%
  group_by(deployment_id, day_id, date) %>%
  summarise(n = n(), .groups = "drop") %>%
  group_by(deployment_id) %>%
  arrange(date) %>% # Sort by actual date, not day_id string
  mutate(
    # For each unique day, check if there's a day before and after
    prev_date = lag(date),
    next_date = lead(date),
    has_prev = !is.na(prev_date),
    has_next = !is.na(next_date),
    # Mark as complete only if it has both
    complete_day = has_prev & has_next
  ) %>%
  select(deployment_id, day_id, complete_day)

# Join back to main dataset
abundance_index_filtered <- abundance_index_filtered %>%
  left_join(unique_days_per_deployment, by = c("deployment_id", "day_id")) %>%
  # Add AR_start column for autocorrelation modeling
  # TRUE for first observation of each day, FALSE otherwise
  arrange(day_id, timestamp) %>%
  group_by(day_id) %>%
  mutate(AR_start = row_number() == 1) %>%
  ungroup()

# Manual patch for specific day_ids that were incorrectly classified
days_to_patch <- c("SC1-20231120", "SC10-20240106", "SC10-20240131", "SC9-20240129")

abundance_index_filtered <- abundance_index_filtered %>%
  mutate(
    complete_day = case_when(
      day_id %in% days_to_patch ~ TRUE,
      TRUE ~ complete_day
    )
  )

# Summary of complete vs partial days
complete_summary <- abundance_index_filtered %>%
  group_by(complete_day) %>%
  summarise(
    n_observations = n(),
    n_unique_days = n_distinct(day_id),
    .groups = "drop"
  )

cat("Complete days summary:\n")
print(complete_summary)
```

# Step 4: Load Temperature Data

```{r}
#| label: load-temperature

# Load temperature data if available
if (file.exists("data/temperature_data_2023.csv")) {
  temperature_data <- read_csv("data/temperature_data_2023.csv", show_col_types = FALSE)
  cat("Loaded", nrow(temperature_data), "temperature records\n")
} else {
  temperature_data <- tibble(filename = character(), temperature = numeric())
  cat("No temperature data found\n")
}
```

# Step 5: Integrate All Data Sources

```{r}
#| label: integrate-data

# Create combined analysis dataframe with butterfly_abundance as base
analysis_df <- abundance_index_filtered %>%
  left_join(
    temperature_data %>%
      select(filename, temperature),
    by = c("image_filename" = "filename")
  ) %>%
  left_join(
    deployments %>%
      select(deployment_id, wind_meter_name, 
             horizontal_dist_to_cluster_m, height_m, Observer, view_id),
    by = "deployment_id"
  )

# Check for missing data
missing_temp <- sum(is.na(analysis_df$temperature))
missing_wind_meter <- sum(is.na(analysis_df$wind_meter_name))

cat("Records missing temperature data:", missing_temp, "\n")
cat("Records missing wind meter assignment:", missing_wind_meter, "\n")
```

# Step 6: Add Wind Features

```{r}
#| label: wind-features

# Function to calculate wind features for each butterfly observation
calculate_wind_features <- function(df, wind_data) {
  
  # Initialize empty list to store wind features for each row
  wind_features_list <- list()
  
  # Process each row in the dataframe
  for(i in 1:nrow(df)) {
    row <- df[i,]
    
    # Skip if no wind meter associated
    if(is.na(row$wind_meter_name)) {
      wind_features_list[[i]] <- tibble(
        wind_mean = NA_real_,
        wind_max_gust = NA_real_,
        wind_sd = NA_real_,
        gust_differential_mean = NA_real_,
        cumulative_wind = NA_real_,
        n_strong_gusts = NA_real_,
        time_above_threshold = NA_real_,
        n_wind_records = NA_real_
      )
      next
    }
    
    # Define time window (30 minutes before the observation)
    end_time <- row$timestamp
    start_time <- end_time - minutes(30)
    
    # For first observation of a day (AR_start == TRUE), use next 30 minutes
    if(!is.na(row$AR_start) && row$AR_start == TRUE) {
      start_time <- end_time
      end_time <- start_time + minutes(30)
    }
    
    # Filter wind data for this meter and time window
    wind_subset <- wind_data %>%
      filter(
        wind_meter_name == row$wind_meter_name,
        time >= start_time,
        time < end_time
      )
    
    # Calculate features if we have wind data
    if(nrow(wind_subset) > 0) {
      features <- wind_subset %>%
        mutate(gust_differential = gust - speed) %>%
        summarize(
          wind_mean = mean(speed, na.rm = TRUE),
          wind_max_gust = max(gust, na.rm = TRUE),
          wind_sd = sd(speed, na.rm = TRUE),
          gust_differential_mean = mean(gust_differential, na.rm = TRUE),
          cumulative_wind = sum(speed, na.rm = TRUE),
          n_strong_gusts = sum(gust > WIND_THRESHOLD, na.rm = TRUE),
          time_above_threshold = sum(speed > WIND_THRESHOLD, na.rm = TRUE),
          n_wind_records = n()
        )
      wind_features_list[[i]] <- features
    } else {
      # No wind data available for this time window
      wind_features_list[[i]] <- tibble(
        wind_mean = NA_real_,
        wind_max_gust = NA_real_,
        wind_sd = NA_real_,
        gust_differential_mean = NA_real_,
        cumulative_wind = NA_real_,
        n_strong_gusts = NA_real_,
        time_above_threshold = NA_real_,
        n_wind_records = 0
      )
    }
    
    # Progress indicator
    if(i %% 100 == 0) {
      cat("Processed", i, "of", nrow(df), "observations\n")
    }
  }
  
  # Combine all wind features into a single dataframe
  wind_features_df <- bind_rows(wind_features_list)
  
  # Add wind features to original dataframe
  cbind(df, wind_features_df)
}

# Apply wind feature calculation
cat("Calculating wind features...\n")
analysis_df <- calculate_wind_features(analysis_df, wind_all)
cat("Wind features calculation complete\n")
```

# Step 7: Final Processing

```{r}
#| label: final-processing

# Add proportion of butterflies in direct sunlight
analysis_df <- analysis_df %>%
  mutate(
    proportion_butterflies_direct_sun = case_when(
      is.na(total_butterflies) | is.na(butterflies_direct_sun) ~ NA_real_,
      total_butterflies == 0 ~ 0,
      TRUE ~ butterflies_direct_sun / total_butterflies
    )
  )

# Filter to complete days only and remove rows with missing wind data
analysis_df_complete <- analysis_df %>%
  filter(complete_day == TRUE) %>%
  filter(!is.na(wind_mean))

# Remove unnecessary columns for final dataset
analysis_df_final <- analysis_df_complete %>%
  select(-date, -complete_day, -wind_meter_name, 
         -horizontal_dist_to_cluster_m, -height_m, 
         -n_wind_records, -n_strong_gusts, -timestamp_str)

cat("Final dataset:", nrow(analysis_df_final), "observations\n")
cat("Unique deployments:", n_distinct(analysis_df_final$deployment_id), "\n")
cat("Unique complete days:", n_distinct(analysis_df_final$day_id), "\n")
```

# Step 8: Data Validation

```{r}
#| label: validation
#| eval: !expr SHOW_VALIDATION_PLOTS

if (SHOW_VALIDATION_PLOTS) {
  
  # 1. Data completeness check
  cat("=== DATA COMPLETENESS ===\n")
  completeness <- analysis_df_final %>%
    summarise(
      total_rows = n(),
      missing_temperature = sum(is.na(temperature)),
      missing_wind_mean = sum(is.na(wind_mean)),
      missing_butterflies = sum(is.na(total_butterflies)),
      invalid_proportions = sum(proportion_butterflies_direct_sun > 1, na.rm = TRUE)
    )
  print(completeness)
  
  # 2. Distribution plots
  cat("\n=== KEY DISTRIBUTIONS ===\n")
  
  p1 <- ggplot(analysis_df_final, aes(x = total_butterflies)) +
    geom_histogram(bins = 30, fill = "steelblue", alpha = 0.7) +
    labs(title = "Total Butterflies Distribution", x = "Total Butterflies", y = "Count")
  print(p1)
  
  p2 <- ggplot(analysis_df_final, aes(x = wind_mean)) +
    geom_histogram(bins = 30, fill = "lightblue", alpha = 0.7) +
    labs(title = "Wind Speed Distribution", x = "Mean Wind Speed (m/s)", y = "Count")
  print(p2)
  
  p3 <- ggplot(analysis_df_final, aes(x = temperature)) +
    geom_histogram(bins = 30, fill = "red", alpha = 0.7) +
    labs(title = "Temperature Distribution", x = "Temperature (°C)", y = "Count")
  print(p3)
  
  # 3. Time coverage
  cat("\n=== TEMPORAL COVERAGE ===\n")
  time_summary <- analysis_df_final %>%
    summarise(
      start_date = min(timestamp, na.rm = TRUE),
      end_date = max(timestamp, na.rm = TRUE),
      total_days = n_distinct(day_id)
    )
  print(time_summary)
  
  # 4. Deployment summary
  cat("\n=== DEPLOYMENT SUMMARY ===\n")
  deployment_summary <- analysis_df_final %>%
    group_by(deployment_id) %>%
    summarise(
      n_observations = n(),
      n_days = n_distinct(day_id),
      start_date = min(as.Date(timestamp)),
      end_date = max(as.Date(timestamp)),
      .groups = "drop"
    ) %>%
    arrange(deployment_id)
  print(deployment_summary)
}
```

# Step 9: Export Final Dataset

```{r}
#| label: export

# Export complete days dataset
output_file <- "data/analysis_complete_days.csv"
write_csv(analysis_df_final, output_file)

cat("=== EXPORT COMPLETE ===\n")
cat("Dataset saved to:", output_file, "\n")
cat("Final dataset dimensions:", nrow(analysis_df_final), "rows x", ncol(analysis_df_final), "columns\n")

# Show structure
cat("\n=== FINAL DATASET STRUCTURE ===\n")
glimpse(analysis_df_final)

# Show first few rows
cat("\n=== SAMPLE DATA ===\n")
print(head(analysis_df_final, 5))
```

# Summary

This script has successfully combined:

- **Deployment metadata** from QGIS and label files
- **Wind data** from SQLite databases with 30-minute window features  
- **Butterfly abundance** from JSON classification files with night filtering and downsampling
- **Temperature data** from trail camera sensors
- **Complete day filtering** ensuring only days with observations before and after are included

The resulting dataset contains **`r nrow(analysis_df_final)`** observations from **`r n_distinct(analysis_df_final$deployment_id)`** deployments across **`r n_distinct(analysis_df_final$day_id)`** complete observation days, ready for downstream analysis.